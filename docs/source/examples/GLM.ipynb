{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a702d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e5d54",
   "metadata": {},
   "source": [
    "\n",
    "# Generalized Linear Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8486656",
   "metadata": {},
   "source": [
    "## Logistic Regressions\n",
    "\n",
    "Logistic regression is a important model to solve classification problem, which is expressed specifically as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& P(y=1 \\mid x)=\\frac{1}{1+\\exp \\left(-x^T \\beta\\right)}, \\\\\n",
    "& P(y=0 \\mid x)=\\frac{1}{1+\\exp \\left(x^T \\beta\\right)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\beta$ is an unknown parameter vector that to be estimated. Since we expect only a few explanatory variables contribute for predicting $y$, we assume $\\beta$ is sparse vector with sparsity level $s$.\n",
    "\n",
    "With $n$ independent data of the explanatory variables $x$ and the response variable $y$, we can estimate $\\beta$ by minimizing the negative log-likelihood function under sparsity constraint:\n",
    "$$\n",
    "\\arg \\min _{\\beta \\in R^p} L(\\beta):=-\\frac{1}{n} \\sum_{i=1}^n\\left\\{y_i x_i^T \\beta-\\log \\left(1+\\exp \\left(x_i^T \\beta\\right)\\right)\\right\\}, \\text { s.t. }\\|\\beta\\|_0 \\leq s\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec381f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True support set:  [2 4 8]\n",
      "Estimated support set:  [2 7 8]\n",
      "True parameters:  [ 0.          0.          0.95008842  0.         -0.10321885  0.\n",
      "  0.          0.         -0.15135721  0.        ]\n",
      "True loss value:  0.6396969\n",
      "Estimated parameters:  [ 0.          0.          0.86291105  0.          0.          0.\n",
      "  0.         -0.32276162  0.23823929  0.        ]\n",
      "Estimated loss value:  0.60980606\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scope import ScopeSolver\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def data_generator(n, p, s, rho, random_state=None):\n",
    "    \"\"\"\n",
    "    * $\\beta^*_i$ ~ N(0, 1), $\\forall i \\in supp(\\beta^*)$\n",
    "    * $x = (x_1, \\cdots, x_p)^T$, $x_{i+1}=\\rho x_i+\\sqrt{1-\\rho^2}z_i$, where $x_1, z_i$ ~ N(0, 1)\n",
    "    * $y\\in\\{0,1\\}$, $P(y=0)=\\frac{1}{1+\\exp^{x^T\\beta^*+c}}$\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    # beta\n",
    "    beta = np.zeros(p)\n",
    "    true_support_set = np.random.choice(p, s, replace=False)\n",
    "    beta[true_support_set] = np.random.normal(0, 1, s)\n",
    "    # X\n",
    "    X = np.empty((n, p))\n",
    "    X[:, 0] = np.random.normal(0, 1, n)\n",
    "    for j in range(1, p):\n",
    "        X[:, j] = rho * X[:, j - 1] + np.sqrt(1-rho**2) * np.random.normal(0, 1, n)\n",
    "    # y\n",
    "    xbeta = np.clip(X @ beta, -30, 30)\n",
    "    p = 1 / (1 + np.exp(-xbeta))\n",
    "    y = np.random.binomial(1, p)\n",
    "\n",
    "    return X, y, beta, true_support_set\n",
    "\n",
    "n, p, s, rho = 100, 10, 3, 0.0\n",
    "X, y, true_params, true_support_set = data_generator(n, p, s, rho , 0)\n",
    "# Define function to calculate negative log-likelihood of logistic regression\n",
    "def logistic_loss(params):\n",
    "    xbeta = jnp.clip(X @ params, -30, 30)\n",
    "    return jnp.mean(jnp.log(1 + jnp.exp(xbeta)) - y * xbeta)\n",
    "\n",
    "solver = ScopeSolver(p, s)\n",
    "solver.solve(logistic_loss, jit=True)\n",
    "\n",
    "print(\"True support set: \", np.sort(true_support_set))\n",
    "print(\"Estimated support set: \", np.sort(solver.support_set))\n",
    "print(\"True parameters: \", true_params)\n",
    "print(\"True loss value: \", logistic_loss(true_params))\n",
    "print(\"Estimated parameters: \", solver.params)\n",
    "print(\"Estimated loss value: \", logistic_loss(solver.params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509eeb3",
   "metadata": {},
   "source": [
    "## Poisson Regression\n",
    "Poisson Regression involves regression models in which the response variable is in the form of counts.\n",
    "For example, the count of number of car accidents or number of customers in line at a reception desk.\n",
    "The response variables is assumed to follow a Poisson distribution.\n",
    "\n",
    "The general mathematical equation for Poisson regression is\n",
    "\n",
    "\\begin{align}\\log(E(y)) = \\beta_0 + \\beta_1 X_1+\\beta_2 X_2+\\dots+\\beta_p X_p.\\end{align}\n",
    "\n",
    "With $n$ independent data of the explanatory variables $x$ and the response variable $y$, we can estimate $\\beta$ by minimizing the negative log-likelihood function under sparsity constraint:\n",
    "$$\n",
    "\\arg \\min _{\\beta \\in R^p} L(\\beta):=-\\frac{1}{n} \\sum_{i=1}^n\\left\\{y_i x_i^T \\beta-\\exp \\left(x_i^T \\beta\\right)-\\log  \\left(y!\\right)\\right\\}, \\text { s.t. }\\|\\beta\\|_0 \\leq s .\n",
    "$$\n",
    "\n",
    "Here is Python code for solving sparse poisson regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0159e431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True support set:  [0 5 9]\n",
      "True parameters:  [4.70030694 0.         0.         0.         0.         8.30570366\n",
      " 0.         0.         0.         3.78436768]\n",
      "True loss value:  0.5956122\n",
      "Estimated support set:  [3 5 9]\n",
      "Estimated parameters:  [ 0.          0.          0.         -3.99304304  0.          8.65190727\n",
      "  0.          0.          0.          4.95582619]\n",
      "Estimated loss value:  0.5782885\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from abess.datasets import make_glm_data\n",
    "import jax.numpy as jnp\n",
    "from scope import ScopeSolver\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100\n",
    "p = 10\n",
    "s = 3\n",
    "data = make_glm_data(n=n, p=p, k=s, family=\"poisson\")\n",
    "X = data.x\n",
    "y = data.y\n",
    "# Define function to calculate negative log-likelihood of poisson regression\n",
    "def poisson_loss(params):\n",
    "    xbeta = jnp.clip(X @ params, -30, 30)\n",
    "    return jnp.mean(jnp.exp(xbeta) - y * xbeta) #omit \\log y! term\n",
    "\n",
    "\n",
    "solver = ScopeSolver(p, s)\n",
    "solver.solve(poisson_loss, jit=True)\n",
    "\n",
    "print(\"True support set: \", np.nonzero(data.coef_)[0])\n",
    "print(\"True parameters: \", data.coef_)\n",
    "print(\"True loss value: \", poisson_loss(data.coef_))\n",
    "print(\"Estimated support set: \", np.sort(solver.support_set))\n",
    "print(\"Estimated parameters: \", solver.params)\n",
    "print(\"Estimated loss value: \", poisson_loss(solver.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dcc923",
   "metadata": {},
   "source": [
    "## Gamma Regression\n",
    "Gamma regression can be used when you have positive continuous response variables such as payments for insurance claims,\n",
    "or the lifetime of a redundant system.\n",
    "It is well known that the density of Gamma distribution can be represented as a function of\n",
    "a mean parameter ($\\mu$) and a shape parameter ($\\alpha$), respectively,\n",
    "$$\n",
    "\\begin{align}f(y \\mid \\mu, \\alpha)=\\frac{1}{y \\Gamma(\\alpha)}\\left(\\frac{\\alpha y}{\\mu}\\right)^{\\alpha} e^{-\\alpha y / \\mu} {I}_{(0, \\infty)}(y),\\end{align}\n",
    "$$\n",
    "where $I(\\cdot)$ denotes the indicator function. In the Gamma regression model,\n",
    "response variables are assumed to follow Gamma distributions. Specifically,\n",
    "\n",
    "\\begin{align}y_i \\sim Gamma(\\mu_i, \\alpha),\\end{align}\n",
    "\n",
    "\n",
    "where $1/\\mu_i = x_i^T\\beta$.\n",
    "\n",
    "With $n$ independent data of the explanatory variables $x$ and the response variable $y$, we can estimate $\\beta$ by minimizing the negative log-likelihood function under sparsity constraint:\n",
    "$$\n",
    "\\arg \\min _{\\beta \\in R^p} L(\\beta):=-\\frac{1}{n} \\sum_{i=1}^n\\left\\{-\\alpha \\left( y_i x_i^T \\beta - \\log \\left(x_i^T \\beta\\right)\\right) + \\alpha \\log \\alpha + \\left(\\alpha - 1\\right) \\log y - \\log \\Gamma \\left(\\alpha\\right) \\right\\}, \\text { s.t. }\\|\\beta\\|_0 \\leq s .\n",
    "$$\n",
    "\n",
    "Here is Python code for solving sparse gamma regression problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae245de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True support set:  [2 6 8]\n",
      "True parameters:  [ 0.          0.         16.84626207  0.          0.          0.\n",
      "  9.48390875  0.          7.42158219  0.        ]\n",
      "True loss value:  nan\n",
      "Estimated support set:  []\n",
      "Estimated parameters:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Estimated loss value:  inf\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "n = 100\n",
    "p = 10\n",
    "s = 3\n",
    "data = make_glm_data(n=n, p=p, k=s, family=\"gamma\")\n",
    "X = data.x\n",
    "y = data.y\n",
    "\n",
    "# Define function to calculate negative log-likelihood of Gamma regression\n",
    "def gamma_loss(params):\n",
    "    xbeta = jnp.clip(X @ params, -30, 30)\n",
    "    return jnp.mean(y * xbeta - jnp.log(xbeta)) \n",
    "\n",
    "\n",
    "solver = ScopeSolver(p, s)\n",
    "solver.solve(gamma_loss, jit=True)\n",
    "\n",
    "print(\"True support set: \", np.nonzero(data.coef_)[0])\n",
    "print(\"True parameters: \", data.coef_)\n",
    "print(\"True loss value: \", gamma_loss(data.coef_))\n",
    "print(\"Estimated support set: \", np.sort(solver.support_set))\n",
    "print(\"Estimated parameters: \", solver.params)\n",
    "print(\"Estimated loss value: \", gamma_loss(solver.params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
