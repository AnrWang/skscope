{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a702d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d013f51",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression\n",
    "\n",
    "Multinomial logistic regression is a type of regression analysis used to predict the probabilities of multiple categorical outcomes. It is an extension of binary logistic regression, which is used to predict the probability of a binary outcome.\n",
    "\n",
    "### Mathematical Derivation\n",
    "\n",
    "In multinomial logistic regression, we have multiple categories, denoted by $k=1,2,...,K$. We want to predict the probability of each category given a set of predictor variables $X$. We assume that the probability of each category is a function of the predictor variables, and that the probabilities for each category sum to 1.\n",
    "\n",
    "We can model the probability of each category using the softmax function:\n",
    "\n",
    "$$P(Y=k|X=x) = \\frac{e^{\\beta _{0k} + \\beta _k^TX}}{\\sum_{j=1}^K e^{\\beta _{0j} + \\beta _j^TX}}$$\n",
    "\n",
    "where $Y$ is the categorical outcome, $X$ is the vector of predictor variables, $\\beta _{0k}$ and $\\beta _k$ are the intercept and coefficient vectors for category $k$, and $e$ is the base of the natural logarithm.\n",
    "\n",
    "The softmax function ensures that the probabilities for each category sum to 1. The numerator of the function represents the probability of category $k$, and the denominator represents the sum of the probabilities for all categories.\n",
    "\n",
    "We can estimate the coefficients using maximum likelihood estimation. The likelihood function for multinomial logistic regression is:\n",
    "\n",
    "$$L(\\beta) = \\prod _{i=1}^n \\prod _{k=1}^K P(Y_i=k|X_i=x_i)^{I(Y_i=k)}$$\n",
    "\n",
    "where $n$ is the number of observations, $I(Y_i=k)$ is an indicator function that equals 1 if $Y_i=k$ and 0 otherwise, and $P(Y_i=k|X_i=x_i)$ is the predicted probability of category $k$ for observation $i$.\n",
    "\n",
    "The negative log-likelihood function is:\n",
    "\n",
    "$$-l(\\beta) = -\\sum _{i=1}^n \\sum _{k=1}^K I(Y_i=k) \\log P(Y_i=k|X_i=x_i)$$\n",
    "\n",
    "This is the function that we want to minimize in order to estimate the coefficients. We can use scope algorithm to find the values of $\\beta$ with sparsity constraints that minimize the negative log-likelihood function.\n",
    "\n",
    "Here is Python code for solving sparse gamma regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b19558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter:\n",
      " [[ 0.          0.          0.        ]\n",
      " [-8.96063657 26.41595784  0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.7509806   7.8726573   0.        ]\n",
      " [-2.85160076 -7.90773217  0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.62682389 -3.13024383  0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 4.49347497  1.93505859  0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "real variables' index:\n",
      " {1, 5, 6, 12, 17}\n",
      "Estimated parameter:\n",
      " [[ 0.          0.          0.        ]\n",
      " [-6.79456559  5.96242422  0.83214265]\n",
      " [-1.67460064  1.09597192  0.57862782]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.1350812  -0.63852108 -0.4965583 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.86637195 -0.72462665 -1.1417439 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.04849507 -0.33571337  0.28721813]]\n",
      "Estimated variables' index:\n",
      " {1, 2, 13, 17, 19}\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scope import ScopeSolver\n",
    "import numpy as np\n",
    "from abess.datasets import make_multivariate_glm_data\n",
    "np.random.seed(3)\n",
    "\n",
    "n = 100  # sample size\n",
    "p = 20  # all predictors\n",
    "k = 5   # real predictors\n",
    "m = 3   # number of classes\n",
    "\n",
    "\n",
    "data = make_multivariate_glm_data(n=n, p=p, k=k, family=\"multinomial\", M=m)\n",
    "\n",
    "X = data.x\n",
    "y = data.y\n",
    "# Define function to calculate negative log-likelihood of multinomial logistic regression\n",
    "def softmax(x, axis=None):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    x_max = jnp.amax(x, axis=axis, keepdims=True)\n",
    "    exp_x_shifted = jnp.exp(x - x_max)\n",
    "    return exp_x_shifted / jnp.sum(exp_x_shifted, axis=axis, keepdims=True)\n",
    "\n",
    "def multinomial_loss(params):\n",
    "    beta = params.reshape((p, m))\n",
    "    prob = softmax(X @ beta, axis=1)\n",
    "    return -jnp.sum(y * jnp.log(prob))\n",
    "\n",
    "\n",
    "solver = ScopeSolver(p*m, k, group=[i for i in range(p) for j in range(m)])\n",
    "solver.solve(multinomial_loss, jit=True)\n",
    "\n",
    "\n",
    "print(\"True parameter:\\n\", data.coef_)\n",
    "print('real variables\\' index:\\n', set(np.nonzero(data.coef_)[0]))\n",
    "print(\"Estimated parameter:\\n\", solver.params.reshape((p, m)))\n",
    "print('Estimated variables\\' index:\\n', set(np.nonzero(solver.params.reshape((p, m)))[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
