{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a702d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d013f51",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression\n",
    "\n",
    "Multinomial logistic regression is a type of regression analysis used to predict the probabilities of multiple categorical outcomes. It is an extension of binary logistic regression, which is used to predict the probability of a binary outcome.\n",
    "\n",
    "### Mathematical Derivation\n",
    "\n",
    "In multinomial logistic regression, we have multiple categories, denoted by $k=1,2,...,K$. We want to predict the probability of each category given a set of predictor variables $X$. We assume that the probability of each category is a function of the predictor variables, and that the probabilities for each category sum to 1.\n",
    "\n",
    "We can model the probability of each category using the softmax function:\n",
    "\n",
    "$$P(Y=k|X=x) = \\frac{e^{\\beta _{0k} + \\beta _k^TX}}{\\sum_{j=1}^K e^{\\beta _{0j} + \\beta _j^TX}}$$\n",
    "\n",
    "where $Y$ is the categorical outcome, $X$ is the vector of predictor variables, $\\beta _{0k}$ and $\\beta _k$ are the intercept and coefficient vectors for category $k$, and $e$ is the base of the natural logarithm.\n",
    "\n",
    "The softmax function ensures that the probabilities for each category sum to 1. The numerator of the function represents the probability of category $k$, and the denominator represents the sum of the probabilities for all categories.\n",
    "\n",
    "We can estimate the coefficients using maximum likelihood estimation. The likelihood function for multinomial logistic regression is:\n",
    "\n",
    "$$L(\\beta) = \\prod _{i=1}^n \\prod _{k=1}^K P(Y_i=k|X_i=x_i)^{I(Y_i=k)}$$\n",
    "\n",
    "where $n$ is the number of observations, $I(Y_i=k)$ is an indicator function that equals 1 if $Y_i=k$ and 0 otherwise, and $P(Y_i=k|X_i=x_i)$ is the predicted probability of category $k$ for observation $i$.\n",
    "\n",
    "The negative log-likelihood function is:\n",
    "\n",
    "$$-l(\\beta) = -\\sum _{i=1}^n \\sum _{k=1}^K I(Y_i=k) \\log P(Y_i=k|X_i=x_i)$$\n",
    "\n",
    "This is the function that we want to minimize in order to estimate the coefficients. We can use scope algorithm to find the values of $\\beta$ with sparsity constraints that minimize the negative log-likelihood function.\n",
    "\n",
    "Here is Python code for solving sparse gamma regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6fcb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter:\n",
      " [[  5.44916029  -0.94953634   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ -1.39241163 -12.96678673   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  3.24543565   4.02033588   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ -1.38210809   4.07755579   0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  5.79719104   3.61096451   0.        ]]\n",
      "real variables' index:\n",
      " {0, 3, 7, 10, 19}\n",
      "Estimated parameter:\n",
      " [[ 4.39849021 -2.51001992 -1.88847321]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 3.99449679 -9.68577372  5.69127535]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.93657379  2.00817472 -2.944751  ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-2.31650417  3.43102526 -1.11451944]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 2.54142473  0.82596718 -3.36739435]]\n",
      "Estimated variables' index:\n",
      " {0, 3, 7, 10, 19}\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scope import ScopeSolver\n",
    "import numpy as np\n",
    "from abess.datasets import make_multivariate_glm_data\n",
    "np.random.seed(3)\n",
    "\n",
    "n = 500  # sample size\n",
    "p = 20  # all predictors\n",
    "k = 5   # real predictors\n",
    "m = 3   # number of classes\n",
    "\n",
    "\n",
    "data = make_multivariate_glm_data(n=n, p=p, k=k, family=\"multinomial\", M=m)\n",
    "\n",
    "X = data.x\n",
    "y = data.y\n",
    "\n",
    "def cross_entropy_loss(params):\n",
    "    beta = params.reshape((p, m))\n",
    "    # Compute the logits\n",
    "    logits = jnp.dot(X, beta)\n",
    "\n",
    "    # Compute the softmax probabilities\n",
    "    softmax_probs = jnp.exp(logits) / jnp.sum(jnp.exp(logits), axis=1, keepdims=True)\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = -jnp.mean(jnp.sum(y * jnp.log(softmax_probs), axis=1))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "solver = ScopeSolver(p*(m), k, group=[i for i in range(p) for j in range(m)])\n",
    "solver.solve(cross_entropy_loss, jit=True)\n",
    "\n",
    "\n",
    "print(\"True parameter:\\n\", data.coef_)\n",
    "print('real variables\\' index:\\n', set(np.nonzero(data.coef_)[0]))\n",
    "print(\"Estimated parameter:\\n\", solver.params.reshape((p, m)))\n",
    "print('Estimated variables\\' index:\\n', set(np.nonzero(solver.params.reshape((p, m)))[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
