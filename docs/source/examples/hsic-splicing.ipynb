{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from scope import ScopeSolver\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. positive effect linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 500, 100\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.normal(0, 1, (n, p))\n",
    "beta = np.zeros(p)\n",
    "beta[:3] = [1, 2, 3]\n",
    "y = X @ beta + rng.normal(0, 1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square-Loss Error:  0.001\n"
     ]
    }
   ],
   "source": [
    "def ols_loss(params):\n",
    "    loss = jnp.mean((y - X @ jnp.abs(params))**2)\n",
    "    return loss\n",
    "\n",
    "solver = ScopeSolver(p, sparsity=3)\n",
    "params = solver.solve(ols_loss)\n",
    "print('Square-Loss Error: ', np.sum((jnp.abs(params)-beta)**2).round(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. non-linear feature selection via HSIC-Splicing\n",
    "In this example, we show an application of positive effect linear regression.  \n",
    "We intend to identifying relevant features $X_k, k\\in[p]$ from $X^{n\\times p}$ which has nonlinear dependence of $y\\in\\mathbb{R}^n$.  \n",
    "Motivated by [Yamada et al.](http://www.ms.k.u-tokyo.ac.jp/sugi/2014/HSICLasso.pdf), we consider the following sparse optimization:\n",
    "$$\\min_{\\alpha\\in\\mathbb{R}^p} \\left\\|\\bar{L}-\\sum_{k=1}^p\\alpha_k\\bar{K}^{(k)}\\right\\|_F^2,\\quad\\text{ s.t. } \\|\\alpha\\|_0\\leq s$$\n",
    "where $\\bar{K}^{(k)}=\\Gamma K^{(k)}\\Gamma\\in\\mathbb{R}^{n\\times n}$, $\\bar{L}=\\Gamma L\\Gamma\\in\\mathbb{R}^{n\\times n}$ are centralized Gram matrices and $\\Gamma=I_n-n^{-1}1_n1_n^{\\top}\\in\\mathbb{R}^{n\\times n}$.  \n",
    "The matrices $K_{i,j}^{(k)}=K(X_{i,k}, X_{j,k})$, $L_{i,j}=L(y_i, y_j)$ are generated via kernel $K$ and $L$ respectively.  \n",
    "In the following, we choose both $K$ and $L$ to be Gaussian kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic(X, y, sparsity, gamma_x=0.7, gamma_y=0.7):\n",
    "    n, p = X.shape\n",
    "    Gamma = np.eye(n) - np.ones((n, 1)) @ np.ones((1, n)) / n\n",
    "    L = rbf_kernel(y.reshape(-1, 1), gamma=gamma_y)\n",
    "    L_bar = Gamma @ L @ Gamma\n",
    "    response = L_bar.reshape(-1)\n",
    "    K_bar = np.zeros((n**2, p))\n",
    "    for k in range(p):\n",
    "        x = X[:, k]\n",
    "        tmp = rbf_kernel(x.reshape(-1, 1), gamma=gamma_x)\n",
    "        K_bar[:, k] = (Gamma @ tmp @ Gamma).reshape(-1)\n",
    "    covariate = K_bar\n",
    "\n",
    "    def custom_objective(alpha):\n",
    "        loss = jnp.mean((response - covariate @ jnp.abs(alpha)) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    solver = ScopeSolver(p, sparsity=sparsity)\n",
    "    alpha = solver.solve(custom_objective)\n",
    "    return alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 additive model:\n",
    "$y=-2\\sin(2X_1)+X_2^2+X_3+\\exp(-X_4)+\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p, s = 500, 1000, 4\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.normal(0, 1, (n, p))\n",
    "noise = rng.normal(0, 1, n)\n",
    "y = -2 * np.sin(2 * X[:, 0]) + X[:, 1] ** 2 + X[:, 2] + np.exp(- X[:, 3]) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = hsic(X, y, s)\n",
    "np.nonzero(alpha)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 non-additive model: \n",
    "$y=X_1\\exp(2X_2)+X_3^2+\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p, s = 500, 1000, 3\n",
    "rng = np.random.default_rng(1)\n",
    "X = rng.normal(0, 1, (n, p))\n",
    "noise = rng.normal(0, 1, n)\n",
    "y = X[:, 0] * np.exp(2 * X[:, 1]) + X[:, 2] ** 2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = hsic(X, y, s)\n",
    "np.nonzero(alpha)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scope-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
