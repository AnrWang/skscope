{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448840ef",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81f287",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Linear regression is a regression method that fits a linear relationship on datasets. It assumes a linear relationship between the independent variables and the dependent variable and seeks the optimal linear function to fit the data. \n",
    "Suppose we collect $n$ independent observations for a response variable and $p$ explanatory variables, say $y \\in R^n$ and $X \\in R^{n\\times p}$. Let $\\epsilon_1, \\ldots, \\epsilon_n$ be i.i.d zero-mean random noises and $\\epsilon = (\\epsilon_1, \\ldots, \\epsilon_n)$, the linear model has a form:\n",
    "\n",
    "$$y=X \\beta^{*} +\\epsilon.$$\n",
    "\n",
    "However, when dealing with high-dimensional data, high-dimensional linear regression faces several challenges, such as:\n",
    "\n",
    "- Computational efficiency: As the number of independent variables increases, the computational complexity of the model also increases. Including all independent variables in the model can result in long computation times and high memory usage. Variable selection improves computational efficiency by reducing the number of variables considered.\n",
    "\n",
    "- Feature correlation: In high-dimensional data, there may be strong correlations between independent variables. These redundant features cause model instability and multicollinearity. Variable selection eliminates redundant features with weak correlations to the target variable, reducing the impact of correlation.\n",
    "\n",
    "- Interpretability: In practical applications, interpretability of the model is crucial. Variable selection removes irrelevant or unimportant variables, making the model easier to understand and explain. This allows us to identify the factors that truly influence the target variable.\n",
    "\n",
    "To address these challenges, variable selection is necessary in high-dimensional linear regression. By selecting relevant and meaningful independent variables, we can reduce the complexity of the model, improve predictive performance, and gain better insights into the relationships within the data.\n",
    "\n",
    "We can consider minimizing the loss function under suitable sparse constraint conditions to obtain appropriate parameter estimates. In other words, we can formulate the problem as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\arg\\min & L(\\beta)=\\frac1{2n}\\| y-X\\beta \\|_2^2\n",
    "\\\\\n",
    "&\\text{subject to:} \\; \\| \\beta \\|_0 \\leq s ,\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\| \\beta \\|_0$ represents the $l_0$ norm of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067005ef",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Next, we will consider using `skscope` to optimize the aforementioned problem and compare it with Lasso regularization. Lasso will use 5-fold cross-validation to select the regularization parameter. Lasso [[1]](#refer-anchor-1) is a commonly used regularization technique that automatically selects relevant features and reduces model complexity by introducing a sparsity penalty term, thereby improving model interpretability and generalization capability.\n",
    "\n",
    "- **First, let's consider the case with no intercept.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f74214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skscope import ScopeSolver\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f15997",
   "metadata": {},
   "source": [
    "We will work with a dataset of size $n = 150$ and dimension $p = 30$. Our assumption is that the sample $X$ and noise $\\epsilon$ are both drawn from normal distributions. The true support set of $\\beta$ is $(1, 2, 3, 0, ..., 0)^{\\top}$, consisting of non-zero coefficients in the first three positions. Now, let's proceed to construct the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067623c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 150, 30\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.normal(0, 1, (n, p))\n",
    "beta = np.zeros(p)\n",
    "beta[:3] = [1, 2, 3]\n",
    "y = X @ beta + rng.normal(0, 0.1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b9a98",
   "metadata": {},
   "source": [
    "Next, we consider using scope and Lasso to estimate the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718b9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_loss(params):\n",
    "    loss = jnp.mean((y - X @ params) ** 2)\n",
    "    return loss\n",
    "solver = ScopeSolver(p, sparsity=3)\n",
    "params_scope = solver.solve(ols_loss)\n",
    "\n",
    "lasso_cv = LassoCV(cv=5, fit_intercept=False)\n",
    "lasso_cv.fit(X, y)\n",
    "params_lasso = lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe315418",
   "metadata": {},
   "source": [
    "Subsequently, we compute the residual sum of squares for the estimates obtained from these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2d19ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope:  0.0003\n",
      "lasso:  0.001\n"
     ]
    }
   ],
   "source": [
    "print('scope: ', np.sum((params_scope-beta) ** 2).round(4))\n",
    "print('lasso: ', np.sum((params_lasso-beta) ** 2).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0157d6c",
   "metadata": {},
   "source": [
    "- **Next, let's consider the case with an intercept term.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcb2e8",
   "metadata": {},
   "source": [
    "We are considering a regression model given by $y=\\beta_0^*+X \\beta^{*} +\\epsilon$, where $\\beta_0^*$ represents the intercept term. Next, we will set the intercept term $\\beta_0^*=1$ and utilize the same settings as mentioned earlier to construct the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89af7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 150, 30\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.normal(0, 1, (n, p))\n",
    "beta = np.zeros(p)\n",
    "beta[:3] = [1, 2, 3]\n",
    "y = X @ beta + rng.normal(1, 0.1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beaaeb3",
   "metadata": {},
   "source": [
    "Next, we will use scope and lasso to estimate the parameter $\\beta^{*}$ and the intercept term $\\beta_0^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed42a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv = LassoCV(cv=5)\n",
    "lasso_cv.fit(X, y)\n",
    "intercept_lasso = lasso_cv.intercept_\n",
    "params_lasso = lasso_cv.coef_\n",
    "\n",
    "X = np.hstack((np.ones((n, 1)), X))  \n",
    "solver = ScopeSolver(p + 1, sparsity=4, preselect=0)\n",
    "scope_estimate = solver.solve(ols_loss)\n",
    "intercept_scope = scope_estimate[0]\n",
    "params_scope = scope_estimate[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc419c9",
   "metadata": {},
   "source": [
    "Afterward, we calculate the sum of squared residuals for the estimations of parameters obtained using these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d34ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope:  0.0004\n",
      "lasso:  0.0011\n"
     ]
    }
   ],
   "source": [
    "print('scope: ', np.sum((params_scope-beta) ** 2).round(4))\n",
    "print('lasso: ', np.sum((params_lasso-beta) ** 2).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70b1e8",
   "metadata": {},
   "source": [
    "Below are the absolute differences between the intercept term obtained by these two methods and its actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b94f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope:  0.0082\n",
      "lasso:  0.0106\n"
     ]
    }
   ],
   "source": [
    "print('scope: ', np.abs(intercept_scope - 1).round(4))\n",
    "print('lasso: ', np.abs(intercept_lasso - 1).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ca530",
   "metadata": {},
   "source": [
    "- **Summary**\n",
    "\n",
    "From the results obtained, it can be observed that in both cases with and without an intercept, the residual sum of squares for the estimates obtained from `skscope` is smaller. This indicates that `skscope` can provide an appropriate method for variable selection in high-dimensional linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724d61a",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "<div id=\"refer-anchor-1\"></div>\n",
    "\n",
    "- [1] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1), 267-288."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scope-dev",
   "language": "python",
   "name": "scope-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
