{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448840ef",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81f287",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Linear regression is a regression method that fits a linear relationship on datasets. It assumes a linear relationship between the independent variables and the dependent variable and seeks the optimal linear function to fit the data. \n",
    "Suppose we collect $n$ independent observations for a response variable and $p$ explanatory variables, say $y \\in R^n$ and $X \\in R^{n\\times p}$. Let $\\epsilon_1, \\ldots, \\epsilon_n$ be i.i.d zero-mean random noises and $\\epsilon = (\\epsilon_1, \\ldots, \\epsilon_n)$, the linear model has a form:\n",
    "\n",
    "$$y=X \\beta^{*} +\\epsilon.$$\n",
    "\n",
    "However, when dealing with high-dimensional data, high-dimensional linear regression faces several challenges, such as:\n",
    "\n",
    "- Computational efficiency: As the number of independent variables increases, the computational complexity of the model also increases. Including all independent variables in the model can result in long computation times and high memory usage. Variable selection improves computational efficiency by reducing the number of variables considered.\n",
    "\n",
    "- Feature correlation: In high-dimensional data, there may be strong correlations between independent variables. These redundant features cause model instability and multicollinearity. Variable selection eliminates redundant features with weak correlations to the target variable, reducing the impact of correlation.\n",
    "\n",
    "- Interpretability: In practical applications, interpretability of the model is crucial. Variable selection removes irrelevant or unimportant variables, making the model easier to understand and explain. This allows us to identify the factors that truly influence the target variable.\n",
    "\n",
    "To address these challenges, variable selection is necessary in high-dimensional linear regression. By selecting relevant and meaningful independent variables, we can reduce the complexity of the model, improve predictive performance, and gain better insights into the relationships within the data.\n",
    "\n",
    "We can consider minimizing the loss function under suitable sparse constraint conditions to obtain appropriate parameter estimates. In other words, we can formulate the problem as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\arg\\min & L(\\beta)=\\frac1{2n}\\| y-X\\beta \\|_2^2\n",
    "\\\\\n",
    "&\\text{subject to:} \\; \\| \\beta \\|_0 \\leq s ,\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\| \\beta \\|_0$ represents the $l_0$ norm of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067005ef",
   "metadata": {},
   "source": [
    "### An Example\n",
    "\n",
    "Next, we will consider using `skscope` to optimize the aforementioned problem and compare it with Lasso regularization. Lasso will use 5-fold cross-validation to select the regularization parameter. Lasso [[1]](#refer-anchor-1) is a commonly used regularization technique that automatically selects relevant features and reduces model complexity by introducing a sparsity penalty term, thereby improving model interpretability and generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f74214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skscope import ScopeSolver\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f15997",
   "metadata": {},
   "source": [
    "We will work with a dataset of size $n = 100$ and dimension $p = 30$. Our assumption is that the sample $X$ and noise $\\epsilon$ are both drawn from normal distributions. The true support set of $\\beta$ is $(1, 2, 3, 0, ..., 0)^{\\top}$, consisting of non-zero coefficients in the first three positions. Now, let's proceed to construct the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "067623c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 100, 30\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.normal(0, 1, (n, p))\n",
    "beta = np.zeros(p)\n",
    "beta[:3] = [1, 2, 3]\n",
    "y = X @ beta + rng.normal(0, 0.1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b9a98",
   "metadata": {},
   "source": [
    "Next, we consider using scope and Lasso to estimate the parameters $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "718b9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_loss(params):\n",
    "    loss = jnp.mean((y - X @ params) ** 2)\n",
    "    return loss\n",
    "solver = ScopeSolver(p, sparsity=3)\n",
    "params_scope = solver.solve(ols_loss)\n",
    "\n",
    "lasso_cv = LassoCV(cv=5)\n",
    "lasso_cv.fit(X, y)\n",
    "params_lasso = lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe315418",
   "metadata": {},
   "source": [
    "Subsequently, we compute the residual sum of squares for the estimates obtained from these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a2d19ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope:  0.0002\n",
      "lasso:  0.0008\n"
     ]
    }
   ],
   "source": [
    "print('scope: ', np.sum((params_scope-beta) ** 2).round(4))\n",
    "print('lasso: ', np.sum((params_lasso-beta) ** 2).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ca530",
   "metadata": {},
   "source": [
    "From the results, it is evident that in this particular example, the estimates obtained from `skscope` yield a smaller residual sum of squares. This implies that `skscope` may provide more appropriate results in some cases of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724d61a",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "<div id=\"refer-anchor-1\"></div>\n",
    "\n",
    "- [1] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1), 267-288."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scope-dev",
   "language": "python",
   "name": "scope-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
